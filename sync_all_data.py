import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client
import re

# --- è¨­å®š ---
# ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ç›´æ¥å…¥åŠ›ã™ã‚‹ã‹ã€.envãƒ•ã‚¡ã‚¤ãƒ«ç­‰ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ ã‚¨ãƒ©ãƒ¼: SUPABASE_URL ã¾ãŸã¯ SUPABASE_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    exit(1)

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

def scrape_uver_schedule():
    print("ğŸ“… UVERworldå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰è©³ç´°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’åŒæœŸä¸­...")
    url = "https://www.uverworld.jp/schedule/list/"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        res = requests.get(url, headers=headers, timeout=15)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, 'html.parser')

        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«é …ç›®ã‚’ç‰¹å®š
        items = soup.find_all(['li', 'dl', 'div'], class_=re.compile(r'schedule|list|item'))
        if not items:
            items = soup.find_all('li')

        print(f"ğŸ” è©³ç´°è§£æä¸­... å€™è£œ: {len(items)}ä»¶")

        count = 0
        for item in items:
            # 1. æ—¥ä»˜ã®æŠ½å‡º (202X.XX.XX)
            text_full = item.get_text(" ", strip=True)
            date_match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})', text_full)
            if not date_match:
                continue
            
            # yyyy-mm-dd å½¢å¼ã«æ­£è¦åŒ–
            event_date = f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"

            # 2. ãƒªãƒ³ã‚¯å†…ãƒ†ã‚­ã‚¹ãƒˆã‚’å„ªå…ˆå–å¾—ï¼ˆã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºç²¾åº¦ã‚¢ãƒƒãƒ—ï¼‰
            link_el = item.find('a')
            raw_title = link_el.get_text(" ", strip=True) if link_el else text_full

            # 3. ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ç‰¹å®š
            category = "OTHER"
            cat_text = "OTHER"
            # å…¬å¼ã®ä¸»è¦ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’æ¤œç´¢
            cat_match = re.search(r'(TOUR|LIVE|EVENT|RELEASE|TV|RADIO|MAGAZINE|GOODS|TICKET|INFO)', raw_title.upper())
            
            if cat_match:
                cat_text = cat_match.group(1)
                if cat_text in ["TOUR", "LIVE", "EVENT"]: category = "LIVE"
                elif cat_text == "RELEASE": category = "RELEASE"
                elif cat_text in ["TV", "RADIO", "MAGAZINE"]: category = "TV"
            
            # 4. ã‚¿ã‚¤ãƒˆãƒ«ã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆå¼·åŒ–ç‰ˆï¼‰
            # æ—¥ä»˜éƒ¨åˆ† (2026.02.13ç­‰) ã‚’å‰Šé™¤
            clean_title = raw_title.replace(date_match.group(0), "")
            
            # æ›œæ—¥ [TUE] ã‚„ (ç«) ï¼»WEDï¼½ ãªã©ã‚’å®Œå…¨ã«é™¤å»
            clean_title = re.sub(r'[\[\(\ï¼ˆ\ï¼»].*?[\]\)\ï¼‰\ï¼½]', '', clean_title)
            
            # ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒ¯ãƒ¼ãƒ‰ (TOURç­‰) ãŒæ–‡é ­ã«ã‚ã‚‹å ´åˆã«å‰Šé™¤
            clean_title = re.sub(r'^(TOUR|LIVE|EVENT|RELEASE|TV|RADIO|MAGAZINE|GOODS|TICKET|INFO)\s*', '', clean_title, flags=re.IGNORECASE)
            
            # æ®‹ã£ã¦ã—ã¾ã£ãŸè¨˜å·ï¼ˆ- ã‚„ :ï¼‰ã‚’æƒé™¤
            clean_title = clean_title.strip(" -:ï¼š")
            
            # ä½™åˆ†ãªç©ºç™½ã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
            clean_title = " ".join(clean_title.split()).strip()

            # ç©ºã«ãªã£ãŸå ´åˆã®ä¿é™º
            if not clean_title: 
                clean_title = f"{cat_text} (è©³ç´°ä¸æ˜)"

            # 5. é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæ—¥ä»˜ã¨ã‚¿ã‚¤ãƒˆãƒ«ã§åˆ¤å®šï¼‰
            existing = supabase.table("calendar_events") \
                .select("*") \
                .eq("event_date", event_date) \
                .eq("title", clean_title) \
                .execute()
            
            if not existing.data:
                supabase.table("calendar_events").insert({
                    "event_date": event_date,
                    "category": category,
                    "title": clean_title,
                    "description": f"Official Category: {cat_text}"
                }).execute()
                print(f"ğŸ†• è¿½åŠ  [{category}]: {event_date} - {clean_title}")
                count += 1
            else:
                # æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã®ãƒ­ã‚°ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                # print(f"â© ã‚¹ã‚­ãƒƒãƒ—: {event_date} - {clean_title}")
                pass

        print(f"\nâœ¨ åŒæœŸå®Œäº†ï¼ æ–°è¦è¿½åŠ : {count} ä»¶")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    scrape_uver_schedule()