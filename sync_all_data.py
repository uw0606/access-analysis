import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client
import re

# --- è¨­å®š ---
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

def scrape_uver_schedule():
    print("ğŸ“… UVERworldå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰è©³ç´°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’åŒæœŸä¸­...")
    url = "https://www.uverworld.jp/schedule/list/"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        res = requests.get(url, headers=headers, timeout=15)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, 'html.parser')

        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«é …ç›®ã‚’ç‰¹å®š
        items = soup.find_all(['li', 'dl'], class_=re.compile(r'schedule|list|item'))
        if not items:
            items = soup.find_all('li')

        print(f"ğŸ” è©³ç´°è§£æä¸­... å€™è£œ: {len(items)}ä»¶")

        count = 0
        for item in items:
            # 1. æ—¥ä»˜ã®æŠ½å‡º (202X.XX.XX)
            text_full = item.get_text(" ", strip=True)
            date_match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})', text_full)
            if not date_match:
                continue
            event_date = f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"

            # 2. ã‚«ãƒ†ã‚´ãƒªãƒ¼ã¨ã‚¿ã‚¤ãƒˆãƒ«ã®è©³ç´°æŠ½å‡º
            # <a>ã‚¿ã‚°ã®ä¸­ã«ã‚ã‚‹å…·ä½“çš„ãªæƒ…å ±ã‚’å„ªå…ˆçš„ã«å–å¾—
            link_el = item.find('a')
            if link_el:
                # <a>å†…ã®æ§‹é€ ã‚’ã•ã‚‰ã«åˆ†è§£
                # å¤šãã®å ´åˆã€span(ã‚«ãƒ†ã‚´ãƒª) + p(ã‚¿ã‚¤ãƒˆãƒ«) ã‚„ ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰
                raw_title = link_el.get_text(" ", strip=True)
            else:
                raw_title = text_full

            # 3. ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ç‰¹å®šã¨ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
            category = "OTHER"
            cat_text = "OTHER"
            cat_match = re.search(r'(TOUR|LIVE|EVENT|RELEASE|TV|RADIO|MAGAZINE|GOODS|TICKET)', raw_title.upper())
            
            if cat_match:
                cat_text = cat_match.group(1)
                if cat_text in ["TOUR", "LIVE", "EVENT"]: category = "LIVE"
                elif cat_text == "RELEASE": category = "RELEASE"
                elif cat_text in ["TV", "RADIO", "MAGAZINE"]: category = "TV"
            
            # 4. ã‚¿ã‚¤ãƒˆãƒ«ã®æ•´å½¢ (æ—¥ä»˜ã‚„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã€æ›œæ—¥ãªã©ã‚’å‰Šé™¤)
            clean_title = raw_title.replace(date_match.group(0), "")
            # æ›œæ—¥ [TUE] ãªã©ã‚’å‰Šé™¤
            clean_title = re.sub(r'\[.*?\]', '', clean_title)
            # ã‚«ãƒ†ã‚´ãƒªãƒ¼å (TOURç­‰) ã‚’å‰Šé™¤
            if cat_text in clean_title.upper():
                clean_title = re.sub(re.escape(cat_text), '', clean_title, flags=re.IGNORECASE)
            
            # ä½™åˆ†ãªç©ºç™½ã‚„æ”¹è¡Œã‚’æƒé™¤
            clean_title = " ".join(clean_title.split()).strip()

            if not clean_title: clean_title = f"{cat_text} (è©³ç´°ä¸æ˜)"

            # 5. é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæ—¥ä»˜ã¨ã‚¿ã‚¤ãƒˆãƒ«ã§åˆ¤å®šï¼‰
            existing = supabase.table("calendar_events").select("*").eq("event_date", event_date).eq("title", clean_title).execute()
            
            if not existing.data:
                supabase.table("calendar_events").insert({
                    "event_date": event_date,
                    "category": category,
                    "title": clean_title,
                    "description": f"Official Category: {cat_text}"
                }).execute()
                print(f"ğŸ†• è¿½åŠ  [{category}]: {event_date} - {clean_title}")
                count += 1

        print(f"\nâœ¨ åŒæœŸå®Œäº†ï¼ æ–°è¦ {count} ä»¶")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    scrape_uver_schedule()